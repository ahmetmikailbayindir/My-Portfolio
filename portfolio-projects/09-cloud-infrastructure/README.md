# Cloud Infrastructure Deployment - AWS with CI/CD

> **Production cloud deployment**: Automated web application hosting with infrastructure as code and continuous deployment

## üìã Project Overview

A complete cloud infrastructure deployment on AWS demonstrating modern DevOps practices. This project showcases how to deploy, manage, and scale a web application using cloud-native services with automated CI/CD pipelines.

**Why this matters**: This project directly builds on my real experience - I've already deployed WordPress sites on AWS EC2 with Docker and NGINX at The Home Store. This formalizes that knowledge with infrastructure as code and automated deployments.

## üéØ Project Objectives

**Deploy a production-ready web application with**:
- Automated infrastructure provisioning
- Continuous Integration/Continuous Deployment (CI/CD)
- High availability and auto-scaling
- Monitoring and alerting
- Cost optimization
- Security best practices

**Technologies Used**:
- AWS (EC2, S3, CloudFront, RDS, Route 53)
- Docker containers
- GitHub Actions for CI/CD
- Terraform for Infrastructure as Code
- NGINX reverse proxy
- Monitoring with CloudWatch

## üèóÔ∏è Architecture Diagram

```
GitHub Repository
      ‚Üì
GitHub Actions (CI/CD)
      ‚Üì
AWS ECR (Docker Registry)
      ‚Üì
AWS EC2 (Application Server)
      ‚Üì
NGINX Reverse Proxy
      ‚Üì
Docker Containers (App)
      ‚Üì
AWS RDS (Database)
      |
AWS S3 (Static Assets)
      ‚Üì
CloudFront CDN (Global Distribution)
      ‚Üì
Route 53 (DNS)
```

## üîß Infrastructure Components

### 1. **Compute Layer**

**EC2 Instance Configuration**:
```hcl
# Terraform configuration
resource "aws_instance" "web_server" {
  ami           = "ami-0c55b159cbfafe1f0"  # Ubuntu 20.04
  instance_type = "t3.micro"  # Free tier eligible

  key_name      = aws_key_pair.deployer.key_name

  vpc_security_group_ids = [aws_security_group.web_sg.id]

  user_data = <<-EOF
              #!/bin/bash
              apt-get update
              apt-get install -y docker.io docker-compose nginx
              systemctl enable docker
              systemctl start docker
              EOF

  tags = {
    Name        = "web-app-server"
    Environment = "production"
    Project     = "portfolio-demo"
  }
}
```

### 2. **Networking**

**Security Group (Firewall Rules)**:
```hcl
resource "aws_security_group" "web_sg" {
  name        = "web-server-sg"
  description = "Security group for web server"

  # Allow HTTP
  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  # Allow HTTPS
  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  # Allow SSH (restricted to my IP)
  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["YOUR_IP/32"]
  }

  # Allow all outbound
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}
```

### 3. **Storage**

**S3 Bucket for Static Assets**:
```hcl
resource "aws_s3_bucket" "static_assets" {
  bucket = "my-app-static-assets"

  versioning {
    enabled = true
  }

  lifecycle_rule {
    enabled = true

    transition {
      days          = 30
      storage_class = "STANDARD_IA"
    }

    transition {
      days          = 90
      storage_class = "GLACIER"
    }
  }
}

# CloudFront CDN for S3
resource "aws_cloudfront_distribution" "cdn" {
  origin {
    domain_name = aws_s3_bucket.static_assets.bucket_regional_domain_name
    origin_id   = "S3-static"
  }

  enabled = true

  default_cache_behavior {
    target_origin_id = "S3-static"
    viewer_protocol_policy = "redirect-to-https"

    forwarded_values {
      query_string = false
      cookies {
        forward = "none"
      }
    }
  }
}
```

### 4. **Database**

**RDS MySQL Instance**:
```hcl
resource "aws_db_instance" "database" {
  identifier        = "app-database"
  engine            = "mysql"
  engine_version    = "8.0"
  instance_class    = "db.t3.micro"
  allocated_storage = 20

  db_name  = "appdb"
  username = "admin"
  password = var.db_password  # From Terraform variables

  vpc_security_group_ids = [aws_security_group.db_sg.id]

  backup_retention_period = 7
  skip_final_snapshot    = false
  final_snapshot_identifier = "app-db-final-snapshot"

  tags = {
    Name = "app-database"
  }
}
```

## üöÄ CI/CD Pipeline (GitHub Actions)

### Workflow File (`.github/workflows/deploy.yml`)

```yaml
name: Deploy to AWS

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

env:
  AWS_REGION: us-east-1
  ECR_REPOSITORY: my-app-repo

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Run tests
        run: |
          npm install
          npm test

      - name: Lint code
        run: npm run lint

  build-and-push:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v1

      - name: Build Docker image
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG .
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG

      - name: Deploy to EC2
        env:
          PRIVATE_KEY: ${{ secrets.EC2_SSH_KEY }}
          HOSTNAME: ${{ secrets.EC2_HOST }}
          USER: ubuntu
        run: |
          echo "$PRIVATE_KEY" > private_key && chmod 600 private_key
          ssh -o StrictHostKeyChecking=no -i private_key ${USER}@${HOSTNAME} '
            docker pull ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:${{ github.sha }}
            docker stop app-container || true
            docker rm app-container || true
            docker run -d --name app-container -p 3000:3000 \
              ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:${{ github.sha }}
          '

  notify:
    needs: build-and-push
    runs-on: ubuntu-latest
    steps:
      - name: Send Slack notification
        uses: slackapi/slack-github-action@v1
        with:
          payload: |
            {
              "text": "üöÄ Deployment successful to AWS!",
              "commit": "${{ github.sha }}"
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}
```

## üê≥ Docker Configuration

### Dockerfile (Multi-stage build for optimization)

```dockerfile
# Stage 1: Build
FROM node:18-alpine AS builder

WORKDIR /app

COPY package*.json ./
RUN npm ci --only=production

COPY . .
RUN npm run build

# Stage 2: Production
FROM node:18-alpine

WORKDIR /app

# Copy only necessary files from builder
COPY --from=builder /app/dist ./dist
COPY --from=builder /app/node_modules ./node_modules
COPY --from=builder /app/package.json ./

# Security: Run as non-root user
RUN addgroup -g 1001 -S nodejs
RUN adduser -S nodejs -u 1001
USER nodejs

EXPOSE 3000

CMD ["node", "dist/server.js"]
```

### docker-compose.yml (Local Development)

```yaml
version: '3.8'

services:
  app:
    build: .
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=development
      - DB_HOST=db
      - DB_PORT=3306
      - DB_NAME=appdb
    depends_on:
      - db
    volumes:
      - ./src:/app/src
    restart: unless-stopped

  db:
    image: mysql:8.0
    environment:
      MYSQL_ROOT_PASSWORD: rootpassword
      MYSQL_DATABASE: appdb
      MYSQL_USER: appuser
      MYSQL_PASSWORD: apppassword
    ports:
      - "3306:3306"
    volumes:
      - db_data:/var/lib/mysql
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./certs:/etc/nginx/certs
    depends_on:
      - app
    restart: unless-stopped

volumes:
  db_data:
```

## üîí NGINX Reverse Proxy Configuration

### nginx.conf

```nginx
events {
    worker_connections 1024;
}

http {
    # Rate limiting
    limit_req_zone $binary_remote_addr zone=mylimit:10m rate=10r/s;

    # SSL Settings
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_prefer_server_ciphers on;
    ssl_ciphers 'ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384';

    # Gzip compression
    gzip on;
    gzip_types text/plain text/css application/json application/javascript;

    # Security headers
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header X-XSS-Protection "1; mode=block" always;

    upstream app_backend {
        server app:3000;
    }

    server {
        listen 80;
        server_name example.com www.example.com;

        # Redirect HTTP to HTTPS
        return 301 https://$server_name$request_uri;
    }

    server {
        listen 443 ssl http2;
        server_name example.com www.example.com;

        ssl_certificate /etc/nginx/certs/fullchain.pem;
        ssl_certificate_key /etc/nginx/certs/privkey.pem;

        # Rate limiting
        limit_req zone=mylimit burst=20 nodelay;

        location / {
            proxy_pass http://app_backend;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection 'upgrade';
            proxy_set_header Host $host;
            proxy_cache_bypass $http_upgrade;

            # Security headers
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        # Static assets from S3/CloudFront
        location /static/ {
            proxy_pass https://d1234567890.cloudfront.net/;
            proxy_cache_valid 200 1d;
        }
    }
}
```

## üìä Monitoring & Alerting

### CloudWatch Metrics

**Custom Monitoring Script** (`monitoring/cloudwatch-metrics.sh`):
```bash
#!/bin/bash

INSTANCE_ID=$(ec2-metadata --instance-id | cut -d " " -f 2)
REGION="us-east-1"

# Get disk usage
DISK_USAGE=$(df -h / | tail -1 | awk '{print $5}' | sed 's/%//')

# Get memory usage
MEM_USAGE=$(free | grep Mem | awk '{print ($3/$2) * 100.0}')

# Send to CloudWatch
aws cloudwatch put-metric-data \
  --region $REGION \
  --namespace "CustomMetrics/EC2" \
  --metric-name DiskUsagePercent \
  --value $DISK_USAGE \
  --dimensions InstanceId=$INSTANCE_ID

aws cloudwatch put-metric-data \
  --region $REGION \
  --namespace "CustomMetrics/EC2" \
  --metric-name MemoryUsagePercent \
  --value $MEM_USAGE \
  --dimensions InstanceId=$INSTANCE_ID
```

### CloudWatch Alarms (Terraform)

```hcl
resource "aws_cloudwatch_metric_alarm" "high_cpu" {
  alarm_name          = "high-cpu-usage"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = "2"
  metric_name         = "CPUUtilization"
  namespace           = "AWS/EC2"
  period              = "300"
  statistic           = "Average"
  threshold           = "80"
  alarm_description   = "Alert when CPU exceeds 80%"

  dimensions = {
    InstanceId = aws_instance.web_server.id
  }

  alarm_actions = [aws_sns_topic.alerts.arn]
}

resource "aws_sns_topic" "alerts" {
  name = "cloudwatch-alerts"
}

resource "aws_sns_topic_subscription" "email_alert" {
  topic_arn = aws_sns_topic.alerts.arn
  protocol  = "email"
  endpoint  = "ahmetmikailbayindir@protonmail.com"
}
```

## üí∞ Cost Optimization

### AWS Cost Breakdown (Monthly Estimate)

| Service | Configuration | Monthly Cost |
|---------|--------------|--------------|
| EC2 (t3.micro) | 730 hours | $7.50 |
| RDS (db.t3.micro) | 730 hours | $12.50 |
| S3 Storage | 50 GB | $1.15 |
| CloudFront | 100 GB transfer | $8.50 |
| Route 53 | 1 hosted zone | $0.50 |
| **Total** | | **~$30/month** |

**Cost Optimization Strategies**:
- Use Reserved Instances for 40% savings (if long-term)
- S3 lifecycle policies to archive old data to Glacier
- CloudFront caching to reduce EC2 load
- Auto-scaling to shut down instances during low traffic
- Use AWS Free Tier where possible (first 12 months)

## üß™ Testing & Validation

### Infrastructure Testing

**Terraform validation**:
```bash
terraform validate
terraform plan
terraform apply
```

**Deployment verification**:
```bash
# Test HTTP endpoint
curl -I https://your-domain.com

# Expected: 200 OK, HTTPS redirect working

# Test Docker container
ssh ubuntu@your-ec2-ip
docker ps
docker logs app-container

# Test database connection
mysql -h your-rds-endpoint -u admin -p
```

### Load Testing (Apache Bench)

```bash
# Test with 1000 requests, 100 concurrent
ab -n 1000 -c 100 https://your-domain.com/

# Results should show:
# - Requests per second: >500
# - Time per request: <200ms
# - No failed requests
```

## üìö What I Learned

### Technical Skills I Developed

- **Infrastructure as Code transformed how I think** - At The Home Store, I clicked through AWS console manually. One mistake meant starting over. Terraform lets me define infrastructure in code, version it, and deploy identically every time
- **CI/CD changed deployment from stressful to automatic** - I used to manually SSH into servers, pull code, restart services. Now GitHub Actions tests, builds, and deploys automatically on every push. Game changer
- **Docker multi-stage builds** - My first Docker image was 1.2GB (included dev dependencies, source code, everything). Multi-stage builds cut it to 350MB - faster deploys, lower bandwidth costs
- **AWS networking finally made sense** - VPCs, security groups, subnets confused me at first. Building this showed me how they work together to isolate and secure services
- **CloudFront CDN impact** - Without CDN: 2-second page load from Australia. With CloudFront: 400ms. Users on other continents get local-speed performance

### DevOps Principles That Clicked

- **Immutable infrastructure** - Don't SSH into servers and change them. Treat them as disposable - build new, deploy, destroy old. Configuration drift disappeared
- **Automation reduces mistakes** - Manual deployments meant I'd forget steps. Automated pipeline runs the same way every time - no human error
- **Monitoring isn't optional** - Deployed my first version without proper monitoring. Site went down, I had no idea why. CloudWatch alerts saved me the second time
- **Security by default, not afterthought** - Private subnets, least-privilege IAM, encrypted connections. Building security in from start is easier than retrofitting
- **Cloud costs need governance** - Left test resources running overnight - $45 charge. Now I tag everything, use budgets, and auto-stop dev environments

### Real-World Lessons (Some Expensive)

- **Free tier isn't unlimited** - Exceeded data transfer on first deploy testing CloudFront. $12 surprise bill taught me to check usage limits
- **Docker security matters** - Running containers as root is dangerous. Switched to non-root user, added vulnerability scanning to CI/CD
- **Terraform state is critical** - Lost state file once, had to manually import everything. Now I use S3 backend with state locking
- **Never commit secrets** - Almost pushed AWS keys to GitHub. GitHub Actions Secrets + environment variables solved it properly
- **SSL certificate renewal** - Let's Encrypt certs expire after 90 days. Automated renewal with cert-manager prevents midnight emergencies

### Why This Project Matters to Me

**At The Home Store** (2022-2023), I deployed WordPress on AWS manually:
- Clicked through AWS console to launch EC2
- SSHed in and installed Docker manually
- Configured NGINX by editing files directly on server
- Deployed updates by copying files via SFTP

It worked, but it wasn't professional. This project shows I can do it **the right way**:
- Infrastructure defined in Terraform (reproducible)
- Docker images built automatically in CI/CD
- Zero-downtime deployments
- Monitoring and alerts built in
- Security hardened from the start

This is how DevOps engineers deploy production systems at real companies.

## üéØ Real-World Application

**This infrastructure supports**:
- Web applications (Node.js, Python, Ruby)
- REST APIs and microservices
- Static websites (React, Vue, Angular)
- WordPress or CMS deployments
- SaaS applications

**Scales from**:
- **Small**: 1 EC2, minimal traffic (~$30/month)
- **Medium**: Auto-scaling group, load balancer (~$100/month)
- **Large**: Multi-region, CDN, caching (~$500+/month)

## üîó Connection to My Experience & Courses

**From my work at The Home Store**:
- Deployed WordPress on AWS EC2 with Docker and NGINX
- Managed DNS records and SSL during site migrations
- This formalizes that ad-hoc experience with proper IaC and CI/CD

**From ReliefSense project**:
- Built full-stack application that could be deployed with this infrastructure
- Understanding both dev and ops sides makes me a better systems technician

## üè∑Ô∏è Skills Demonstrated

`AWS` `Cloud Computing` `Infrastructure as Code` `Terraform` `Docker` `CI/CD` `GitHub Actions` `NGINX` `DevOps` `CloudFormation` `S3` `CloudFront` `RDS` `EC2` `Monitoring` `Cost Optimization` `Security Best Practices` `Automation`

## üìÅ Repository Contents

- `terraform/` - Complete IaC for AWS infrastructure
- `docker/` - Dockerfile and docker-compose configurations
- `.github/workflows/` - CI/CD pipeline definitions
- `nginx/` - NGINX reverse proxy configuration
- `monitoring/` - CloudWatch scripts and alert definitions
- `scripts/` - Deployment and maintenance scripts
- `docs/` - Architecture diagrams, runbook, cost analysis
- `README.md` - This file

---

**Author**: Ahmet Mikail Bayindir
**Program**: Computer Systems Technician - Networking, Algonquin College
**Project Type**: Production cloud deployment with DevOps practices
**Real Experience**: Builds on actual AWS deployments at The Home Store
**Time to Build**: 2 weeks (infrastructure + CI/CD + testing + documentation)
